{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb17cf38-fb14-4c9e-94a5-5c3d0390d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e5905-e137-4fb4-baa1-737fd4bd6bf0",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "This loads experimental and Monte Carlo data, extracts the first 8 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1620d4ca-8a0e-4f2c-9181-0d52df27a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/spin/RUS_Extended_Exp/Preprocess/exp_data_filtered.npy\"   \n",
    "independent_data_path = \"/Users/spin/RUS_Extended_MC/Preprocess/mc_comb_data.npy\"   \n",
    "\n",
    "exp_data = np.load(data_path)\n",
    "exp_8d = exp_data[:, 0:8]\n",
    "mc_data = np.load(independent_data_path)\n",
    "mc_8d = mc_data[:, 0:8]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f9478-ee42-4059-959a-e120b9078368",
   "metadata": {},
   "source": [
    "### Data Scaling and Tensor Conversion\n",
    "\n",
    "This combines the experimental and Monte Carlo data, scales them using `StandardScaler`, and converts them into PyTorch tensors. It also creates condition flags (`0` for MC data and `1` for experimental data) and combines the data and condition flags for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5049e4c3-9bd6-4379-88f8-add1afb2cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experimental data shape: torch.Size([239870, 8])\n",
      "Loaded MC (source) data shape: torch.Size([1467839, 8])\n"
     ]
    }
   ],
   "source": [
    "#Scale Data Together----\n",
    "combined = np.concatenate([mc_8d, exp_8d], axis=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(combined)\n",
    "mc_scaled = scaler.transform(mc_8d)\n",
    "exp_scaled = scaler.transform(exp_8d)\n",
    "mc_tensor = torch.tensor(mc_scaled, dtype=torch.float32, device=device)\n",
    "exp_tensor = torch.tensor(exp_scaled, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"Loaded experimental data shape:\", exp_tensor.shape)  # (N, 8)\n",
    "print(\"Loaded MC (source) data shape:\", mc_tensor.shape)     # (N, 8)\n",
    "\n",
    "# Create condition flags: 0 for source (MC), 1 for target (experimental)\n",
    "cond_mc = torch.zeros((mc_tensor.shape[0], 1), device=device)\n",
    "cond_exp = torch.ones((exp_tensor.shape[0], 1), device=device)\n",
    "X_combined = torch.cat([mc_tensor, exp_tensor], dim=0)\n",
    "cond_combined = torch.cat([cond_mc, cond_exp], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41a300-1b42-41a1-a5e6-ea8d9b50037a",
   "metadata": {},
   "source": [
    "### Define Custom Conditional Affine Coupling Transform and Build Normalizing Flow\n",
    "\n",
    "This defines a custom conditional affine coupling transform (`ConditionalAffineCoupling`) using a neural network to compute scaling and translation parameters. It also defines a simple permutation transform (`Permute`) for reordering features. \n",
    "\n",
    "The `create_conditional_flow` function constructs a conditional normalizing flow by stacking multiple conditional affine coupling transforms and permutation layers to form the desired flow model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9749777-10ff-4404-9167-b6bc4bbeb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalAffineCoupling(nn.Module):\n",
    "    \"\"\"\n",
    "    A conditional affine coupling transform.\n",
    "    Splits input x into two halves, concatenates the condition to the first half,\n",
    "    and uses an DNN to compute scaling (s) and translation (t) parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim, condition_dim=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.condition_dim = condition_dim\n",
    "        # The network takes (dim//2 + condition_dim) inputs and outputs 2*(dim//2) parameters.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim // 2 + condition_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2 * (dim // 2))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        # Split x into two parts along the last dimension.\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        # Concatenate x1 with the condition.\n",
    "        x1_cond = torch.cat([x1, cond], dim=-1)\n",
    "        theta = self.net(x1_cond)\n",
    "        s, t = theta.chunk(2, dim=-1)\n",
    "        s = torch.clamp(s, min=-5, max=5)\n",
    "        y2 = x2 * torch.exp(s) + t\n",
    "        y = torch.cat([x1, y2], dim=-1)\n",
    "        log_det = s.sum(dim=-1)\n",
    "        return y, log_det\n",
    "\n",
    "    def inverse(self, y, cond):\n",
    "        # Inverse transformation.\n",
    "        y1, y2 = y.chunk(2, dim=-1)\n",
    "        y1_cond = torch.cat([y1, cond], dim=-1)\n",
    "        theta = self.net(y1_cond)\n",
    "        s, t = theta.chunk(2, dim=-1)\n",
    "        x2 = (y2 - t) * torch.exp(-s)\n",
    "        x = torch.cat([y1, x2], dim=-1)\n",
    "        log_det = -s.sum(dim=-1)\n",
    "        return x, log_det\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple permutation transform that reorders features.\n",
    "    \"\"\"\n",
    "    def __init__(self, permutation):\n",
    "        super().__init__()\n",
    "        # Store permutation and its inverse as buffers.\n",
    "        self.register_buffer(\"permutation\", permutation)\n",
    "        self.register_buffer(\"inverse_permutation\", torch.argsort(permutation))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, self.permutation]\n",
    "\n",
    "    def inv(self, x):\n",
    "        return x[:, self.inverse_permutation]\n",
    "\n",
    "def create_conditional_flow(dim, num_layers=4, hidden_dim=128, condition_dim=1):\n",
    "    transforms = []\n",
    "    for _ in range(num_layers):\n",
    "        coupling = ConditionalAffineCoupling(dim, hidden_dim, condition_dim)\n",
    "        transforms.append(coupling)\n",
    "        perm = torch.randperm(dim)\n",
    "        permute = Permute(perm)\n",
    "        transforms.append(permute)\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fe022-7455-4e9e-9fdd-c87fe939181e",
   "metadata": {},
   "source": [
    "### Build and Train Conditional Normalizing Flow\n",
    "\n",
    "1. **Model Setup:**\n",
    "   - A base distribution is defined as an 8-dimensional standard normal distribution.\n",
    "   - A conditional normalizing flow is built using `create_conditional_flow`, which consists of 12 layers, each containing an affine coupling transform and a permutation.\n",
    "\n",
    "2. **Helper Functions:**\n",
    "   - `conditional_forward`: Applies the forward pass of the normalizing flow, returning the transformed data and the log-determinant.\n",
    "   - `conditional_inverse`: Applies the inverse pass, returning the original data and the log-determinant.\n",
    "   - `conditional_log_prob`: Computes the log-probability of the data under the normalizing flow.\n",
    "\n",
    "3. **Training Function:**\n",
    "   - `train_conditional_flow`: A training loop that optimizes the normalizing flow using the Adam optimizer. It minimizes the negative log-probability of the data using backpropagation and gradient clipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098232ea-afa0-43e6-8a10-fd30e83f25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8  # Eight features \n",
    "num_layers = 12 # flow layers\n",
    "hidden_dim = 128\n",
    "transforms = create_conditional_flow(dim, num_layers, hidden_dim, condition_dim=1)\n",
    "\n",
    "# Define a base distribution: 8D standard normal using torch.distributions.\n",
    "base_dist = D.Independent(\n",
    "    D.Normal(torch.zeros(dim, device=device), torch.ones(dim, device=device)),\n",
    "    1\n",
    ")\n",
    "\n",
    "def conditional_forward(x, cond, transforms):\n",
    "    log_det_total = torch.zeros(x.shape[0], device=x.device)\n",
    "    z = x\n",
    "    for transform in transforms:\n",
    "        if isinstance(transform, ConditionalAffineCoupling):\n",
    "            z, log_det = transform.forward(z, cond)\n",
    "        else:\n",
    "            # Permutation transform; log-det is 0.\n",
    "            z = transform(x=z)\n",
    "            log_det = 0\n",
    "        log_det_total += log_det\n",
    "    return z, log_det_total\n",
    "\n",
    "def conditional_inverse(z, cond, transforms):\n",
    "    log_det_total = torch.zeros(z.shape[0], device=z.device)\n",
    "    x = z\n",
    "    # Apply inverse transforms in reverse order.\n",
    "    for transform in reversed(transforms):\n",
    "        if isinstance(transform, ConditionalAffineCoupling):\n",
    "            x, log_det = transform.inverse(x, cond)\n",
    "        else:\n",
    "            x = transform.inv(x)\n",
    "            log_det = 0\n",
    "        log_det_total += log_det\n",
    "    return x, log_det_total\n",
    "\n",
    "def conditional_log_prob(x, cond, base_dist, transforms):\n",
    "    z, log_det = conditional_forward(x, cond, transforms)\n",
    "    return base_dist.log_prob(z) + log_det\n",
    "\n",
    "\n",
    "def train_conditional_flow(transforms, base_dist, data_tensor, cond_tensor,\n",
    "                           epochs=8950, batch_size=512, lr=1e-3, grad_clip=1.0):\n",
    "    params = []\n",
    "    for transform in transforms:\n",
    "        if isinstance(transform, ConditionalAffineCoupling):\n",
    "            params += list(transform.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    losses = []\n",
    "    num_samples = data_tensor.shape[0]\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        indices = torch.randint(0, num_samples, (batch_size,))\n",
    "        x_batch = data_tensor[indices]\n",
    "        cond_batch = cond_tensor[indices]\n",
    "        loss = -conditional_log_prob(x_batch, cond_batch, base_dist, transforms).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, grad_clip)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c81d4-36af-4422-b3fb-a31bd1145d6f",
   "metadata": {},
   "source": [
    "### Train the Conditional Flow and Morph Simulation to Experimental Data\n",
    "\n",
    "1. **Training the Conditional Flow:**\n",
    "   - The `train_conditional_flow` function is used to train the normalizing flow model on the combined MC (source) and experimental (target) data. The training process minimizes the negative log-likelihood to morph the source (MC) data distribution to match the target (experimental) data distribution.\n",
    "\n",
    "2. **Morphing Simulation (MC) Data to Experimental Data:**\n",
    "   - The function `morph_simulation_to_data` is used to transform the MC data to match the experimental data by passing through the trained normalizing flow. The source data is first passed through the forward pass, and then an inverse pass is applied to convert it into the target (experimental) distribution.\n",
    "\n",
    "3. **Scaling Recovery:**\n",
    "   - After morphing the MC data to experimental data, the scaling applied to the data earlier is reversed using the `scaler.inverse_transform` method, recovering the original units of both MC and experimental data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b375a977-24a2-4c48-9e2c-826e6fa849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Training conditional flow on combined MC (source) and experimental (target) data...\")\n",
    "# train_losses = train_conditional_flow(transforms, base_dist, X_combined, cond_combined)\n",
    "# @torch.no_grad()\n",
    "\n",
    "# def morph_simulation_to_data(x_source, transforms):\n",
    "#     # Use condition 0 for source (MC) and 1 for target (experimental)\n",
    "#     cond_source = torch.zeros((x_source.shape[0], 1), device=x_source.device)\n",
    "#     cond_target = torch.ones((x_source.shape[0], 1), device=x_source.device)\n",
    "#     z, _ = conditional_forward(x_source, cond_source, transforms)\n",
    "#     x_corrected, _ = conditional_inverse(z, cond_target, transforms)\n",
    "#     return x_corrected\n",
    "\n",
    "# corrected_samples_scaled = morph_simulation_to_data(mc_tensor, transforms)\n",
    "# corrected_samples_scaled = corrected_samples_scaled.cpu().numpy()\n",
    "\n",
    "# # Inverse transform the scaling to recover original units.\n",
    "# mc_data_orig = scaler.inverse_transform(mc_tensor.cpu().numpy())\n",
    "# exp_data_orig = scaler.inverse_transform(exp_tensor.cpu().numpy())\n",
    "# corrected_samples = scaler.inverse_transform(corrected_samples_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d130ac58-0fa1-4564-a6e3-ab7d0ab76ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(p_samples, q_samples, bins=100):\n",
    "    \"\"\"\n",
    "    KL(P || Q): p_samples is the true (experimental), q_samples is morphed MC.\n",
    "    \"\"\"\n",
    "    kl_total = 0\n",
    "    for dim in range(p_samples.shape[1]):\n",
    "        p_hist, bin_edges = np.histogram(p_samples[:, dim], bins=bins, density=True)\n",
    "        q_hist, _ = np.histogram(q_samples[:, dim], bins=bin_edges, density=True)\n",
    "        \n",
    "        # Avoid zero division & normalize\n",
    "        p_hist += 1e-10\n",
    "        q_hist += 1e-10\n",
    "        p_hist /= np.sum(p_hist)\n",
    "        q_hist /= np.sum(q_hist)\n",
    "\n",
    "        kl = entropy(p_hist, q_hist)\n",
    "        kl_total += kl\n",
    "    return kl_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0840956c-e814-4f30-84b3-d3110be3e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    @torch.no_grad()\n",
    "    def morph_simulation_to_data(x_source, transforms):\n",
    "        cond_source = torch.zeros((x_source.shape[0], 1), device=x_source.device)\n",
    "        cond_target = torch.ones((x_source.shape[0], 1), device=x_source.device)\n",
    "        z, _ = conditional_forward(x_source, cond_source, transforms)\n",
    "        x_corrected, _ = conditional_inverse(z, cond_target, transforms)\n",
    "        return x_corrected\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 4, 16)\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 5e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 2000, 10000)  # adaptive epochs\n",
    "\n",
    "    # Build flow\n",
    "    transforms = create_conditional_flow(dim=8, num_layers=num_layers,\n",
    "                                         hidden_dim=hidden_dim, condition_dim=1)\n",
    "    params = []\n",
    "    for t in transforms:\n",
    "        if isinstance(t, ConditionalAffineCoupling):\n",
    "            params += list(t.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    num_samples = X_combined.shape[0]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        indices = torch.randint(0, num_samples, (batch_size,))\n",
    "        x_batch = X_combined[indices]\n",
    "        cond_batch = cond_combined[indices]\n",
    "        loss = -conditional_log_prob(x_batch, cond_batch, base_dist, transforms).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    corrected_scaled = morph_simulation_to_data(mc_tensor, transforms).cpu().numpy()\n",
    "    corrected_scaled = np.nan_to_num(corrected_scaled, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "    corrected_orig = scaler.inverse_transform(corrected_scaled)\n",
    "\n",
    "    # Compute KL between corrected MC and experimental data\n",
    "    exp_orig = scaler.inverse_transform(exp_tensor.cpu().numpy())\n",
    "    kl = compute_kl_divergence(exp_orig, corrected_orig)\n",
    "\n",
    "    return kl\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best KL divergence:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e074643-96d5-4eee-8080-3b28b727ced0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b453d-d455-4c59-9d5a-a2d05f0ce3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ef635-70ec-48a0-911c-2734c780c8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root_env)",
   "language": "python",
   "name": "root_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
